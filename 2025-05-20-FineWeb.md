---
title: "FineWeb: A Lightweight and Efficient BERT"
date: "2025-04-21"
Link: ""
skills: 
---

# FineWeb: A Lightweight and Efficient BERT

## Dataset Scale
- 15-trillion tokens, 44TB disk space
- Evaluation: Llama architecture (1.82B parameters), GPT2 tokenizer, trained on 28B tokens

## Data Processing Pipeline

### Text Extraction
- WARC format: Contains raw HTML and request metadata
- WET format: Text-only version (found suboptimal)
- Used trafilatura for better quality text extraction

### Deduplication
- MinHash algorithm (scales well on multiple CPU nodes)
- Process:
  - Collect 5-grams â†’ 112 hash functions in 14 buckets
  - Apply 8 hash functions to all 5-grams
  - Documents with identical buckets considered duplicates
- Trade-offs:
  - Positive: Removes duplicate content across dumps (boilerplate, headers)
  - Negative: May upsample unique but low-quality content
  - Note: Only deduplicated within datasets, not across multiple dumps

### Quality Filtering
- Applied C4 dataset filtering criteria:
  - Removed lines not ending with punctuation
  - Filtered javascript and cookie notices
  - Dropped documents outside length thresholds
  - Removed content with "lorem ipsum" or curly brackets
- Terminal punctuation filter: Significant quality boost but removed 30% of tokens (not included)

## FineWeb Edu Variant
- Educational content classifier:
  - Used Llama-3-70B-Instruct to score 500K samples (0-5 scale)
  - Trained a Snowflake-artic-embed classifier on this dataset
- Results:
  - Filtered 15T tokens down to 1.3T educational tokens (92% reduction)
  - Outperforms other open web databases
  - Available as deduplicated 200B token dataset on smollm-corpus
  - Deduplication didn't impact performance

# References
- [HF blog post](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)
- [Smollm-corpus](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus)